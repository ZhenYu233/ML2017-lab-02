#导入相应包
from sklearn.datasets import load_svmlight_file
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
from scipy.linalg.misc import norm

#读取相应数据
data,label=load_svmlight_file('a9a.t')
data = data.todense()

#切分数据集
feature_train,feature_val,label_train,label_val = train_test_split(data,label,test_size=0.25, random_state=42)
#向矩阵中加入偏置项的函数
def add_bias(matrix):    
    bias = []
    #构造偏置项list
    for i in range(matrix.shape[0]):
        bias.append(1)
    #向矩阵中加入一列偏置项
    matrix = np.column_stack((matrix,bias))
    return matrix

#在特征集合中加入偏置项
feature_train = add_bias(feature_train)
feature_val = add_bias(feature_val)
#构造并初始化系数矩阵
w = np.ones((1,123))
#梯度下降算法函数
# x 表示特征训练集
# y 表示标签训练集
# x_val 表示特征验证集
# y_val 表示标签验证集
# w 表示初始化的系数矩阵
# learning_rate 表示学习率
# max_iterations 表示最大迭代次数
def gradient_descent(x,y,x_val,y_val,C,w,learning_rate,max_iterations):
    
    #将label进行转置
    y = np.transpose(np.asmatrix(y))
    y_val = np.transpose(np.asmatrix(y_val))
    w = np.transpose(np.asmatrix(w))
    

    #定义迭代次数和loss值
    iteration_time = [] #迭代次数
    train_loss_value = [] #训练数据的loss值
    val_loss_value = [] #验证数据的loss值

    
    #进行迭代计算
    for i in range(0,max_iterations):
        
        
        #计算SVM损失函数的梯度
        gradient = 0
        for j in range(len(x)):
            if 1 - (y[j].tolist()[0][0])*( np.dot(x[j],w).tolist()[0][0] ) >= 0 :
                gradient = gradient - (y[j].tolist()[0][0])*(x[j])
        
        gradient = gradient*C
        gradient = np.transpose(gradient)+w
        
    
        #对系数矩阵进行更新
        w -= learning_rate*gradient

        
        #定义损失函数并求出损失函数的结果
        #训练集上的loss值
        train_loss = 0
        for j in range(len(x)):
            train_loss = train_loss +  max(0,1 - (y[j].tolist()[0][0])*( np.dot(x[j],w).tolist()[0][0] ))  
        train_loss*=C/x.shape[0]
        train_loss+=((norm(w))**2)/2
        
        #验证集上的loss值
        val_loss = 0
        for j in range(len(x_val)):
            val_loss = val_loss +  max(0,1 - (y_val[j].tolist()[0][0])*( np.dot(x_val[j],w).tolist()[0][0] ))
        val_loss *=C/x_val.shape[0]
        val_loss +=((norm(w))**2)/2

        
        #记录迭代次数和loss的值
        iteration_time.append(i)
        train_loss_value.append(train_loss)
        val_loss_value.append(val_loss)
        
        
    
   
    #画出loss函数图像
    plt.plot(iteration_time, train_loss_value , label='Training Loss')
    plt.plot(iteration_time, val_loss_value , label='Validation Loss')

    plt.title('loss')
    plt.legend()
    plt.show()


gradient_descent(feature_train,label_train,feature_val,label_val,0.5,w,0.001,300)
